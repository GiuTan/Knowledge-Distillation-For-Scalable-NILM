This repository refers to the published method "Knowledge Distillation for Scalable NILM" by Tanoni et al. 2024. DOI: 10.1109/TII.2023.3328436 
Codes for pre-training, fine-tuning and distillation are available. Also, the same pre-training and fine-tuning models are available to be used in the distillation process. 

![image](https://github.com/user-attachments/assets/0afead23-bfc7-423b-9265-568da048cf76)

The approach has been built by using a CRNN.

